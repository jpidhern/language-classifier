{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rLLHK5ryE1H"
      },
      "source": [
        "# Homework 1: Language Identification\n",
        "11-411/11-611 Natural Language Processing (Fall 2024)\n",
        "\n",
        "- RELEASED: Tuesday, Feb 4, 2025\n",
        "- DUE: Tuesday, Feb 18, 2025 11:59 pm EDT\n",
        "\n",
        "**Submission**: Please upload this single file named `HW1.ipynb` to [gradescope](https://www.gradescope.com/courses/557600). Ensure you do not rename the file prior to submission.\n",
        "\n",
        "\n",
        "In this assignment, you will build a language identification classifier that distinguishes between six languages:\n",
        "\n",
        "- [Hausa](https://en.wikipedia.org/wiki/Hausa_language)\n",
        "- [Indonesian](https://en.wikipedia.org/wiki/Indonesian_language)\n",
        "- [Manobo](https://en.wikipedia.org/wiki/Manobo_languages)\n",
        "- [Nahuatl](https://en.wikipedia.org/wiki/Nahuatl)\n",
        "- [Swahili](https://en.wikipedia.org/wiki/Swahili_language)\n",
        "- [Tagalog](https://en.wikipedia.org/wiki/Tagalog_language)\n",
        "\n",
        "Some languages can be distinguished easily, because they use different scripts. These six languages, however, use the same ([Latin](https://en.wikipedia.org/wiki/Latin_script)) script with minimal [diacritics](https://en.wikipedia.org/wiki/Diacritic) so it is difficult to hand-craft classifiers based on the presence or absence of particular characters. Indeed, unless you have linguistic training or familiarity with the languages, it is difficult to tell them apart.\n",
        "\n",
        "How can they be distinguished? A naïve approach is to use word counts as unigram features. However, the number of possible words in a large corpus of five languages is vast. It is essential to look at something smaller — characters.\n",
        "\n",
        "Even though the six languages use roughly the same characters, the relative frequencies of these characters vary greatly. Thus, using characters as features (unigram character models) is appealing (and fairly effective). It is also true that languages vary in their *phonotactics*, the way in which consonants and vowels combine in sequence. Thus, looking at character ngrams (for small values of $n$) is also appealing (and effective). Note, however, that as the value of $n$ increases, this approach runs into the same problem as the word unigram model (sparcity). In this scenario, the model is likely to overfit.\n",
        "\n",
        "Various kinds of classifiers can be used for this application. NB classifiers, for example, are quite effective. However, inference is slow and performance, given the same training set, is likely to be worse than other options. Simple logistic regression cannot be used because this is an n-way (multinomial) classification problem. Multinomial Logistic Regression (Softmax Regression) is a good fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLyf3G0tyE1J"
      },
      "source": [
        "## Requirements\n",
        "\n",
        "You will perform the following tasks:\n",
        "\n",
        "1. Implement a training loop for Multinomial Logistic Regression.\n",
        "2. Implement inference for Multinomal Logistic Regression\n",
        "3. Determine the optimal order of $n$ for ngrams for MNLR trained on the training set.\n",
        "4. Calculate and display a confusion matrix for a trigram model evaluated on the test set.\n",
        "5. Inspect the feature weights, and display the most predictive features for each language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrCw0790yE1J"
      },
      "source": [
        "## How to Use Jupyter Notebooks for Our Assignment\n",
        "Throughout our first assignment, we'll utilize Jupyter Notebooks to walk you through concepts, let you implement them, and also allow you to experiment on your own. By the end of this assignment, you'll not only understand word embeddings more deeply but will also become familiar with a powerful tool used extensively in the data science community.\n",
        "\n",
        "### Types of Cells:\n",
        "**Markdown Cells**: These cells (like the one you're reading now) are utilized to write text, frame explanations, embed images, or even formulate equations. They make our notebook more explanatory and structured.\n",
        "\n",
        "**Code Cells**: This is where the action happens. In these cells, you'll write and execute Python code. They will play a critical role in our exercises as you experiment with word embeddings.\n",
        "\n",
        "*Warning*: Refrain from rearranging, adding, or deleting any cells.\n",
        "\n",
        "### Runtime Volatility\n",
        "As you navigate and execute the cells within this Jupyter notebook, it's crucial to understand that the runtime environment is volatile. In simpler terms:\n",
        "\n",
        "*   If you restart the notebook or experience a disconnection, all your in-memory data and variables will be lost.\n",
        "*   While the code and markdown cells will remain, the outputs from code cells will need to be regenerated by rerunning them.\n",
        "\n",
        "Therefore, if you're working on a task over an extended period or with large datasets, remember to save your results and progress frequently to avoid potential data loss.\n",
        "\n",
        "\n",
        "### Running this Notebook on Google Colab\n",
        "Google Colab is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud. To run this notebook on Colab:\n",
        "\n",
        "1.   Save a copy of this notebook on your **Google Drive**.\n",
        "2.   Open the notebook in Google Colab.\n",
        "3.   In Google Colab, you can execute each cell using the play button (or use the keyboard shortcuts mentioned above).\n",
        "\n",
        "**Tip**: Google Colab may automatically disconnect after a certain period of inactivity. Keep this in mind, especially when running longer tasks.\n",
        "\n",
        "**Note**: Google Colab provides free access to GPUs and TPUs, which might be useful for later assignments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZWqjeNuyE1K"
      },
      "source": [
        "## Imports\n",
        "Do not change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "L2x9fT-5yE1K"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "import numpy.testing as testing\n",
        "\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKs9Yu7nyE1K"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6KiQ1O7yE1K"
      },
      "source": [
        "### Metrics for Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "PXDTvSxGyE1L"
      },
      "outputs": [],
      "source": [
        "def precision(tp: int, fp: int) -> float:\n",
        "    \"\"\"Computes the precision, given true positives and false positives.\"\"\"\n",
        "    return tp / (tp + fp)\n",
        "\n",
        "\n",
        "def recall(tp: int, fn: int) -> float:\n",
        "    \"\"\"Computes the recall, given the true positives and false negatives.\"\"\"\n",
        "    return tp / (tp + fn)\n",
        "\n",
        "\n",
        "def f_measure(beta: float, tp: int, fp: int, fn: int) -> float:\n",
        "    \"\"\"Computes the F-measure for a given beta, true positives, false positives, and false negatives.\"\"\"\n",
        "    return (\n",
        "        (1 + beta**2)\n",
        "        * (precision(tp, fp) * recall(tp, fn))\n",
        "        / (beta**2 * precision(tp, fp) * recall(tp, fn))\n",
        "    )\n",
        "\n",
        "\n",
        "def f1(tp: int, fp: int, fn: int) -> float:\n",
        "    \"\"\"Computes the F1 measure for a given TP, FP, and FN.\"\"\"\n",
        "    return f_measure(1, tp, fp, fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM1mJJs4yE1L"
      },
      "source": [
        "### Micro-Averaged Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "LxugZlTPyE1L"
      },
      "outputs": [],
      "source": [
        "def micro_precision(tp: \"dict[str, int]\", fp: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes micro-averaged precision.\"\"\"\n",
        "    tp_sum = sum(tp.values())\n",
        "    fp_sum = sum(fp.values())\n",
        "    return tp_sum / (tp_sum + fp_sum)\n",
        "\n",
        "\n",
        "def micro_recall(tp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes micro-averaged recall.\"\"\"\n",
        "    tp_sum = sum(tp.values())\n",
        "    fn_sum = sum(fn.values())\n",
        "    return tp_sum / (tp_sum + fn_sum)\n",
        "\n",
        "\n",
        "def micro_f1(tp: \"dict[str, int]\", fp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes micro-averaged F1.\"\"\"\n",
        "    mp = micro_precision(tp, fp)\n",
        "    mr = micro_recall(tp, fn)\n",
        "    return 2 * (mp * mr) / (mp + mr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNG8QyAnyE1L"
      },
      "source": [
        "### Macro-Averaged Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "aOdvkruIyE1L"
      },
      "outputs": [],
      "source": [
        "def macro_precision(tp: \"dict[str, int]\", fp: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes macro-averaged precision.\"\"\"\n",
        "    n = len(tp)\n",
        "    return (1 / n) * sum([precision(tp[c], fp[c]) for c in tp.keys()])\n",
        "\n",
        "\n",
        "def macro_recall(tp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes macro-averaged recall.\"\"\"\n",
        "    n = len(tp)\n",
        "    return (1 / n) * sum([recall(tp[c], fn[c]) for c in tp.keys()])\n",
        "\n",
        "\n",
        "def macro_f1(tp: \"dict[str, int]\", fp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes macro-averaged F1.\"\"\"\n",
        "    n = len(tp)\n",
        "    return (\n",
        "        2\n",
        "        * (macro_precision(tp, fp) * macro_recall(tp, fn))\n",
        "        / (macro_precision(tp, fp) + macro_recall(tp, fn))\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60uRaPkSyE1L"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5JenndHyE1L"
      },
      "source": [
        "### Loading data from files\n",
        "\n",
        "We first need to load data from the provided TSV files. Each file is two columns, the language of the document and the document text, separated by a tab (`\\t`) character. We load this data into a list of tuples, to maintain the coupling between each document and its label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "expfX6fGyE1M"
      },
      "outputs": [],
      "source": [
        "def load_data(data_filename: str) -> list[tuple[str, str]]:\n",
        "    with open(data_filename) as fin:\n",
        "        reader = csv.reader(fin, delimiter=\"\\t\")\n",
        "        language_document_tuples = [(lang, doc) for (lang, doc) in reader]\n",
        "    return language_document_tuples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Y6mfWfyE1M"
      },
      "source": [
        "### Feature Extraction\n",
        "\n",
        "We will use ngrams as features, so we need to be able to extract them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "nQYIOr2NyE1M"
      },
      "outputs": [],
      "source": [
        "def extract_ngrams(x: str, n=3) -> \"list[str]\":\n",
        "    \"\"\"Given a string, return all character ngrams of order `n`.\"\"\"\n",
        "    return [\"\".join(s) for s in (zip(*[x[i:] for i in range(n)]))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkIdpen-yE1M"
      },
      "source": [
        "### Language Codes to One-Hot Vectors\n",
        "And we need a function to convert a language code into a **one-hot vector** (called $\\mathbf{y}$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "RSynlhE0yE1M"
      },
      "outputs": [],
      "source": [
        "def to_onehot_vector(lang: str, langs: list[str]) -> np.ndarray:\n",
        "    y = np.zeros(len(langs))\n",
        "    y[langs.index(lang)] = 1\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZPNsUJ0yE1M"
      },
      "source": [
        "We need to be able to convert `dict`s of ngram counts to vectors of ngram counts (using a map from ngrams to dimensions of the vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "4eLNWl8iyE1M"
      },
      "outputs": [],
      "source": [
        "def vectorize_ngrams(counter: dict[str, int], feature_map: dict[str, int]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Given a dict of ngram counts and a map from features to indices, returns a vector of ngram counts.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    counter : dict\n",
        "        Counter/dict of ngram counts\n",
        "    feature_map : dict\n",
        "        Map from ngrams to indices\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        A vector of ngram counts\n",
        "    \"\"\"\n",
        "    feature_vector = np.zeros(len(feature_map))\n",
        "    for ngram, count in counter.items():\n",
        "        if ngram in feature_map:\n",
        "            feature_vector[feature_map[ngram]] = count\n",
        "    return feature_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-A2mO8_yE1M"
      },
      "source": [
        "And putting together the conversion from text into ngrams, and vectorizing the ngrams:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "AbTp6E5IyE1M"
      },
      "outputs": [],
      "source": [
        "def vectorize_document(document: str, feature_map: dict[str, int], ngram_length: int) -> np.ndarray:\n",
        "    document_ngrams = extract_ngrams(document, ngram_length)\n",
        "    vector = vectorize_ngrams(Counter(document_ngrams), feature_map)\n",
        "    return vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2czxhma2yE1M"
      },
      "source": [
        "We need separate functions for preprocessing the training observations and the dev/test observations. The former function must return a map from language names to labels as one-hot vectors as well as a map from features (ngrams) to indices of vector dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "xJ4N-OLbyE1M"
      },
      "outputs": [],
      "source": [
        "def preprocess_training_observations(\n",
        "    training_observations: list[tuple[str, str]], n: int = 1\n",
        ") -> tuple[list[tuple[np.ndarray, np.ndarray]], dict[str, np.ndarray], dict[str, int]]:\n",
        "    langs = set()\n",
        "    features = set()\n",
        "    obs = []\n",
        "\n",
        "    for lang, doc in training_observations:\n",
        "        langs.add(lang)\n",
        "        ngrams = extract_ngrams(doc, n)\n",
        "        features = features | set(ngrams)\n",
        "        obs.append((lang, ngrams))\n",
        "    feature_map = {feature: idx for idx, feature in enumerate(sorted(features))}\n",
        "    lang_list = list(sorted(langs))\n",
        "    lang_map = {lang: to_onehot_vector(lang, lang_list) for lang in lang_list}\n",
        "\n",
        "    obs = [\n",
        "        (lang_map[lang], vectorize_ngrams(Counter(ngrams), feature_map)) for (lang, ngrams) in obs\n",
        "    ]\n",
        "\n",
        "    print(f\"{len(obs)} training observations.\")\n",
        "    return obs, lang_map, feature_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDikqEOJyE1N"
      },
      "source": [
        "The function for preprocessing test observations (and dev observations) takes the feature map and the language map as arguments and returns only the list of labeled observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "SiMQaZ4gyE1N"
      },
      "outputs": [],
      "source": [
        "def preprocess_test_observations(\n",
        "    test_observations, feature_map: dict[str, int], lang_map: dict[str, np.ndarray], n: int = 1\n",
        ") -> tuple[list[np.ndarray], list[np.ndarray]]:\n",
        "    obs = []\n",
        "\n",
        "    for i, (lang, doc) in enumerate(test_observations):\n",
        "        vectorized_doc = vectorize_document(doc, feature_map, ngram_length=n)\n",
        "        try:\n",
        "            obs.append((lang_map[lang], vectorized_doc))\n",
        "        except KeyError:\n",
        "            print(f\"Unkown language {lang} at index {i}. Known languages are: {lang_map.keys()}\")\n",
        "    print(f\"{len(obs)} test observations.\")\n",
        "    return obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsIWr3b4yE1N"
      },
      "source": [
        "### Classification Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAlUSgMTyE1N"
      },
      "source": [
        "We also need to define the softmax function.\n",
        "\n",
        "Softmax is technically\n",
        "\n",
        "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum^K_{j=1} e^{z_j}}$$\n",
        "\n",
        "However, if implemented naïvely, this is not numerically stable. Instead, we use:\n",
        "\n",
        "$$\\text{softmax}(z_i) = \\frac{e^{z_i - \\text{max}(z)}}{\\sum^K_{j=1} e^{z_j - \\text{max}(z)}}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Dz2KGujgyE1N"
      },
      "outputs": [],
      "source": [
        "def softmax(z: npt.ArrayLike) -> npt.ArrayLike:\n",
        "    \"\"\"Compute the softmax of a vector `z`\"\"\"\n",
        "    # exp(z) can get very large. For numerical stability, we subtract a vector of very large values (np.max(z)) from z.\n",
        "    return np.exp(z - np.max(z)) / np.exp(z - np.max(z)).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmmpqXV2yE1N"
      },
      "source": [
        "## Training the Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzWD7RGCyE1N"
      },
      "source": [
        "### Compute the gradient\n",
        "\n",
        "The formula for computing one element in our gradient is as follows (the partial derivitive of the negative log likelihood loss):\n",
        "\n",
        "$$\\frac{\\partial L_{CE}}{\\partial \\mathbf{w}_{k,i}}=-(\\mathbf{y}_k-\\hat{\\mathrm{y}}_k)\\mathbf{x}_i$$\n",
        "\n",
        "where $k$ is the **class** (rows of the matrix $\\mathbf{w}$) and $i$ corresponds the the feature (columns of the matrix $\\mathbf{w}$).\n",
        "\n",
        "We will define a function `grad` for computing the whole gradient, a $K \\times N$ matrix.\n",
        "\n",
        "**This is the first piece of code that you'll write.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "rUTieBtDyE1N",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [],
      "source": [
        "def grad(W: np.ndarray, y: np.ndarray, x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Caculates the gradient of the negative log liklihood loss, a [K * N] matrix, with respect to W.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    W : np.ndarray\n",
        "        A matrix of of weights, expressed as a [K * N] matrix where K is the number of classes and N is the number of features.\n",
        "    y : np.ndarray\n",
        "       The true label of the observation, expressed as a one-hot encoded vector of shape [K].\n",
        "    x : np.ndarray\n",
        "        A vector of features.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        The gradient of the loss with respect to W.\n",
        "    \"\"\"\n",
        "    x = np.insert(x,0,[1])\n",
        "    # biasCol = np.array([])\n",
        "    #get the first col of W.\n",
        "    # for i in range(0,len(W)):\n",
        "    #   biasCol = np.append(biasCol, W[i][0])\n",
        "    # biasCol = biasCol.reshape(-1, 1)\n",
        "    sx = softmax(np.dot(W,x))\n",
        "    # sx = sx.T\n",
        "    # sx = sx.flatten()\n",
        "    #print(sx, \"this is the softmax\")\n",
        "    #print(np.subtract(y,sx), \"one-hot - softmax\")\n",
        "    lossY = np.subtract(y,sx)\n",
        "    # print(lossY)\n",
        "    gradient = np.tensordot(lossY, x, 0) * -1\n",
        "    # gradientFull = np.array([[]])\n",
        "    # for k in range(0, len(y)):\n",
        "    #   gradientPartial = []\n",
        "    #   for i in range(0,len(x)):\n",
        "    #     gradientElt = -1 * (np.subtract(y[k],sx[k])) * x[i]\n",
        "    #     gradientPartial = np.append(gradientPartial, gradientElt)\n",
        "    #   #print(gradientPartial, \" yeah, this is going in my gradient\")\n",
        "    #   gradientFull = np.append(gradientFull, gradientPartial)\n",
        "    # gradient = np.reshape(gradientFull, (-1, len(x)))\n",
        "    #tensor add/subtract\n",
        "    #How do we know what to update in W? What ARE we updating?\n",
        "    #print(gradient, \" this is the gradient\")\n",
        "    return gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsWLPhtnyE1N"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "Iterate over the observations in the training set $n$ times (in random order). For each item, compute the one-hot vector $\\mathbf{y}$ and the probability distribution $\\hat{\\mathbf{y}}$. Use these values to compute the gradient. Update the parameters based on the gradient. At the end of each epoch (pass through the training data), compute the true positives, false positives, and false negatives for each target class based on the current weights, and report micro-averaged precision and recall.\n",
        "\n",
        "**Note**: The way that we evaluated the training loop depended on an in-place shuffle of the training data, which, while not technically wrong, is maybe not the most intuitive way to do it. When shuffling your data, please either use a shuffling method that does not change the observations parameter in-place. In particular, please use `random.sample` like:\n",
        "```\n",
        "shuffled_observations = random.sample(observations, len(observations))\n",
        "```\n",
        "\n",
        "How you report the metrics is up to you — we will not look at what you output to STDOUT — but it is important that you do this. **Otherwise, you will not be able to determine whether your model is training.**\n",
        "\n",
        "You can also output the loss at each step (very noisy!), each epoch, or run the classifier on the dev set and report the metrics at the end of each epoch.\n",
        "\n",
        "Remember that the 0th column in $W$ contains the biases. You will have to insert a $1$ at the beginning of the feature vector $x$ in order to accomodate this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "qt8b3a7dyE1N",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [],
      "source": [
        "def train(observations: tuple[np.ndarray, np.ndarray], eta: float, epochs: int = 1) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Give a set of observations, returns a trained multinomial LR (softmax regression) model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    observations : (np.ndarray, np.ndarray)\n",
        "        Pairs consisting of a tuple of NumPy arrays (a one-hot vector encoding the ground truth language labels and a vector of features)\n",
        "    epochs : int\n",
        "        The number of epochs to train the model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        The moodel as a [K * N] weight matrix.\n",
        "    \"\"\"\n",
        "    tp = {\"hausa\": 0, \"indonesian\": 0, \"manobo\": 0, \"nahuatl\": 0,\"swahili\": 0, \"tagalog\": 0}\n",
        "    #not yet updated\n",
        "    fp = {\"hausa\": 0, \"indonesian\": 0, \"manobo\": 0, \"nahuatl\": 0,\"swahili\": 0, \"tagalog\": 0}\n",
        "    fn = {\"hausa\": 0, \"indonesian\": 0, \"manobo\": 0, \"nahuatl\": 0,\"swahili\": 0, \"tagalog\": 0}\n",
        "    idxToName = [\"hausa\", \"indonesian\", \"manobo\", \"nahuatl\", \"swahili\", \"tagalog\"]\n",
        "    #These are X by 1 vectors\n",
        "    #print(observations[0])\n",
        "    #These are X by 1 vectors, but these need to be 1 by X vectors.\n",
        "    #print(observations[1])\n",
        "    #The amount of classes is the length of the one hot vector of labels.\n",
        "    K = len(observations[0][0])\n",
        "    #The number of features is 1 + the length of th feature vector (account for biases)\n",
        "    N = len(observations[0][1]) + 1\n",
        "    W = np.zeros((K,N))\n",
        "    #print(W)\n",
        "    for ep in range(0,epochs):\n",
        "      obsLen = len(observations)\n",
        "      shuffled_observations = random.sample(observations, obsLen)\n",
        "      for i in range(0,obsLen):\n",
        "        featuresRow = shuffled_observations[i][1]\n",
        "        features = featuresRow.reshape(-1, 1)\n",
        "        oneHot = shuffled_observations[i][0]\n",
        "        #print(features)\n",
        "        #print(oneHot)\n",
        "        if oneHot[classify(W, features)] == 1:\n",
        "          #can we rely on order being as stated in the writeup?\n",
        "          #need to get from id to string somehow\n",
        "          tp[idxToName[classify(W, features)]] += 1\n",
        "        else:\n",
        "          fp[idxToName[classify(W, features)]] += 1\n",
        "          #find the index of the 1 inthe one hot vector\n",
        "          max = 0\n",
        "          mxidx = -1\n",
        "          for j in range(0,len(oneHot)):\n",
        "            if oneHot[j] > max:\n",
        "              max = oneHot[j]\n",
        "              mxidx = j\n",
        "          fn[idxToName[mxidx]] += 1\n",
        "          #I guess ill insert the 0 to account for the biases here? Do i need to use this above?\n",
        "        gradi = grad(W, oneHot, features)\n",
        "        W = np.subtract(W,(eta * gradi))\n",
        "        #print(W)\n",
        "      mp = micro_precision(tp, fp)\n",
        "      mr = micro_recall(tp, fn)\n",
        "      print(mp, \" is the micro precision \\n\", mr, \"is the micro recall\")\n",
        "    return W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYlrRD2iyE1O"
      },
      "source": [
        "## Classification\n",
        "\n",
        "The classification function is very simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "cXDadimZyE1O",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [],
      "source": [
        "def classify(W: np.ndarray, x: np.ndarray) -> np.intp:\n",
        "    \"\"\"\n",
        "    Return the index of the hypothesized language.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    W : np.ndarray\n",
        "        Weight matrix (one row for each category/language, on column for each feature)\n",
        "    x : np.ndarray\n",
        "        Vector of real-valuled features\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    intp\n",
        "        The index of the hypothesized language.\n",
        "    \"\"\"\n",
        "   # biasCol = np.array([])\n",
        "    x = np.insert(x,0,[1])\n",
        "    #get the first col of W.\n",
        "    #print(len(W))\n",
        "    # for i in range(0,len(W)):\n",
        "    #   biasCol = np.append(biasCol, W[i][0])\n",
        "    #print(np.dot(W,x), \" This is the feature vector when product with the weight matrix\")\n",
        "    #print(biasCol, \" These are our biases\")\n",
        "    # biasCol = biasCol.reshape(-1, 1)\n",
        "    sx = softmax(np.dot(W,x))\n",
        "    #print(sx)\n",
        "    # max = 0\n",
        "    # mxidx = -1\n",
        "    mxidx = np.argmax(sx)\n",
        "    # for i in range(0,len(sx)):\n",
        "    #   if sx[i][0] > max:\n",
        "    #     max = sx[i][0]\n",
        "    #     mxidx = i\n",
        "    #print(mxidx)\n",
        "    return mxidx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvS-HsSyyE1O"
      },
      "source": [
        "## Evaluate the Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVr5HQUzyE1O"
      },
      "source": [
        "Then, a function to train and evaluate the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "JfBmJWlQyE1O"
      },
      "outputs": [],
      "source": [
        "def evaluate(train_set, test_set, eta, epochs=3):\n",
        "    \"\"\"Trains and evaluates a model.\"\"\"\n",
        "    print(\"Training model\")\n",
        "    W = train(train_set, eta, epochs=epochs)\n",
        "    print(\"\\nCLASSIFY\")\n",
        "    tp, fp, fn = Counter(), Counter(), Counter()\n",
        "    for ref_lang_vec, x in test_set:\n",
        "        ref_lang = np.argmax(ref_lang_vec)\n",
        "\n",
        "        hyp_lang = classify(W, x)\n",
        "        if hyp_lang == ref_lang:\n",
        "            tp[ref_lang] += 1\n",
        "        else:\n",
        "            fp[hyp_lang] += 1\n",
        "            fn[ref_lang] += 1\n",
        "    # Print metrics\n",
        "\n",
        "    test_macro_f1 = macro_f1(tp, fp, fn)\n",
        "    test_micro_f1 = micro_f1(tp, fp, fn)\n",
        "    print(f\"macro-averaged F1:\\t\\t{test_macro_f1:.3f}\")\n",
        "    print(f\"micro-averaged F1:\\t\\t{test_micro_f1:.3f}\")\n",
        "    return test_macro_f1, test_micro_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_Yi3u7fyE1O"
      },
      "source": [
        "### Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt6pGjk-yE1O"
      },
      "source": [
        "Load the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "F9ppF8bXyE1O"
      },
      "outputs": [],
      "source": [
        "train_observations = load_data(\"train.tsv\")\n",
        "test_observations = load_data(\"test.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zOiLs2IyE1P"
      },
      "source": [
        "### Inspecting the data\n",
        "\n",
        "Before we actually train the classifier, it's important to look at your data, and check that any assumptions you're making about it are justified. It's always useful at this point to check basic things, like:\n",
        "- How many instances of train and test data do you have?\n",
        "- What labels are in your data, and do those match between the train and test splits?\n",
        "- What is the class balance (i.e. how many instances of each class) in your dataset? Is it balanced or unbalanced?\n",
        "\n",
        "Output a dictionary for the train and test data that maps each label to the count of instances that have that label, e.g.:\n",
        "```\n",
        "{\"hausa\": 4000, \"indonesian\":...}\n",
        "```\n",
        "\n",
        "Your dictionary should be sorted in descending order of occurrence of languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGEpfqiFyE1P",
        "outputId": "2d58ac51-8bf2-4658-f546-2fcd07a3cfca",
        "output_for": "1.1",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hausa': 91, 'indonesian': 86, 'manobo': 111, 'nahuatl': 90, 'swahili': 377, 'tagalog': 365}\n"
          ]
        }
      ],
      "source": [
        "# this cell's output will be used for test 1.1\n",
        "# your code here - train set\n",
        "d = {\"hausa\": 0, \"indonesian\": 0, \"manobo\": 0, \"nahuatl\": 0, \"swahili\": 0, \"tagalog\": 0}\n",
        "idxToName = [\"hausa\", \"indonesian\", \"manobo\", \"nahuatl\", \"swahili\", \"tagalog\"]\n",
        "for k,v in train_observations:\n",
        "  for n in idxToName:\n",
        "    if k == n:\n",
        "      d[k] += 1\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ec4VSKgyE1P",
        "outputId": "3b3ec81d-c1d6-4788-94ba-015a939f4aab",
        "output_for": "1.2",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hausa': 28, 'indonesian': 32, 'manobo': 34, 'nahuatl': 36, 'swahili': 142, 'tagalog': 121}\n"
          ]
        }
      ],
      "source": [
        "# this cell's output will be used for test 1.2\n",
        "# your code here - test set\n",
        "d = {\"hausa\": 0, \"indonesian\": 0, \"manobo\": 0, \"nahuatl\": 0, \"swahili\": 0, \"tagalog\": 0}\n",
        "idxToName = [\"hausa\", \"indonesian\", \"manobo\", \"nahuatl\", \"swahili\", \"tagalog\"]\n",
        "for k,v in test_observations:\n",
        "  for n in idxToName:\n",
        "    if k == n:\n",
        "      d[k] += 1\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rETq5USgyE1P"
      },
      "source": [
        "### Set hyperparameters and parameters\n",
        "\n",
        "Before training the model, we have to set the learning rate $\\eta$ and the order of the ngrams used in feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "d26VTkykyE1P"
      },
      "outputs": [],
      "source": [
        "eta = 0.0005  # Do not change this.\n",
        "epochs = 4  # Do not change this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "JmCmuEpXQ3V7"
      },
      "outputs": [],
      "source": [
        "order_of_ngrams = 2  # Change this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35um7JeyyE1P"
      },
      "source": [
        "### Running our training and evaluation loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7XNyGzmyE1P"
      },
      "source": [
        "Now train your classifier, and evaluate it on the test set. Vary the number of ngrams, and observe how it changes train and test F1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITzdOoUHyE1P",
        "outputId": "c37a4230-17a6-4269-cbc0-3f5890ec8e90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1120 training observations.\n",
            "393 test observations.\n",
            "Training model\n",
            "0.8803571428571428  is the micro precision \n",
            " 0.8803571428571428 is the micro recall\n",
            "0.9299107142857143  is the micro precision \n",
            " 0.9299107142857143 is the micro recall\n",
            "0.9467261904761904  is the micro precision \n",
            " 0.9467261904761904 is the micro recall\n",
            "0.9564732142857143  is the micro precision \n",
            " 0.9564732142857143 is the micro recall\n",
            "\n",
            "CLASSIFY\n",
            "macro-averaged F1:\t\t0.979\n",
            "micro-averaged F1:\t\t0.987\n"
          ]
        }
      ],
      "source": [
        "train_set, lang_map, feature_map = preprocess_training_observations(\n",
        "    train_observations, n=order_of_ngrams\n",
        ")\n",
        "test_set = preprocess_test_observations(test_observations, feature_map, lang_map, n=order_of_ngrams)\n",
        "\n",
        "\n",
        "random.seed(27)\n",
        "test_macro_f1, test_micro_f1 = evaluate(train_set, test_set, eta, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4FNpx3JyE1P"
      },
      "source": [
        "Print the tuple of the best values of `(macro_f1, micro_f1)` for the model evaluated on your test set while varying the order of the ngrams. What value of n produced the best result? Why might that be?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl3218WryE1P",
        "outputId": "023875a4-ecb0-4c7a-dea4-9f8e9eb6763a",
        "output_for": "2.1",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.9793453311893895, 0.9872773536895675)\n"
          ]
        }
      ],
      "source": [
        "# the output of this cell will be used for test 2.1\n",
        "print((test_macro_f1, test_micro_f1))\n",
        "#the value of n that worked the best is 2-ngrams. This may be because these languages use a\n",
        "#\"japanese style\" of writing where 2-letter characters prefixes are representative of meaning\n",
        "#rather than traditional 1 character writing. But i dont think their text is romanized.\n",
        "#So it could laso be because knowing what I know about swahili, there are several common\n",
        "#2 character prefixes and suffixes that characterize the words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADbxCAlLyE1P"
      },
      "source": [
        "## Inspecting Classification Results\n",
        "\n",
        "We've trained our classifier, and your final F1 should be pretty close to 1.0. Great job! But what does that mean for the languages you're actually classifying? Let's rewrite our evaluation code to allow us to look at our results instance-by-instance. For this, we're going to examine the results of a re-trained trigram classifier, trained for one epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eQAaoAYyE1Q",
        "outputId": "b89b590a-20db-4c38-9dfa-f6e46ac10760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1120 training observations.\n",
            "393 test observations.\n",
            "0.8580357142857142  is the micro precision \n",
            " 0.8580357142857142 is the micro recall\n",
            "macro-averaged F1:\t\t0.949\n",
            "micro-averaged F1:\t\t0.962\n"
          ]
        }
      ],
      "source": [
        "# Re-training a trigram classifier. Do not change this.\n",
        "\n",
        "INSPECTION_NGRAMS = 3\n",
        "\n",
        "train_set, lang_map, feature_map = preprocess_training_observations(\n",
        "    train_observations, n=INSPECTION_NGRAMS\n",
        ")\n",
        "test_set = preprocess_test_observations(\n",
        "    test_observations, feature_map, lang_map, n=INSPECTION_NGRAMS\n",
        ")\n",
        "\n",
        "random.seed(27)\n",
        "W_inspect = train(train_set, eta, epochs=1)\n",
        "\n",
        "## evaluate it as before. Check that this looks the same!\n",
        "tp, fp, fn = Counter(), Counter(), Counter()\n",
        "for ref_lang_vec, x in test_set:\n",
        "    ref_lang = np.argmax(ref_lang_vec)\n",
        "\n",
        "    hyp_lang = classify(W_inspect, x)\n",
        "    if hyp_lang == ref_lang:\n",
        "        tp[ref_lang] += 1\n",
        "    else:\n",
        "        fp[hyp_lang] += 1\n",
        "        fn[ref_lang] += 1\n",
        "# Print metrics\n",
        "\n",
        "print(f\"macro-averaged F1:\\t\\t{macro_f1(tp, fp, fn):.3f}\")\n",
        "print(f\"micro-averaged F1:\\t\\t{micro_f1(tp, fp, fn):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21Z8m6YVyE1Q"
      },
      "source": [
        "### Writing a prediction function\n",
        "\n",
        "Write a function that takes a list of observations, and produces a list of either class indices, or class names based on a parameter. This will allow you both to look at individual results from evaluating on an existing set of data (like the test set), but also for you to evaluate your classifier on new data (i.e. any string). This will involve vectorizing the list of documents, and classifying those vectors. Once that's complete, construct an inverse language mapping, from predicted indices to the language they represent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "AQZHWlX7yE1Q",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [],
      "source": [
        "def predict(\n",
        "    documents: list[str],\n",
        "    W: np.ndarray,\n",
        "    feature_map: dict[str, int],\n",
        "    lang_map: dict[str, np.ndarray],\n",
        "    ngram_length: int,\n",
        "    return_class_names=False,\n",
        "):\n",
        "    # your code here\n",
        "    # a dictionary maps the string to a ohv.\n",
        "    #what do i do with calssified feature vectors\n",
        "    #init a list set return a list of the class names, otherwise you return whatever output form classified function\n",
        "    #true => [\"Indonesian\", \"....\"]\n",
        "    # false => just return the indices. [3,5,6...]\n",
        "    idxToName = [\"hausa\", \"indonesian\", \"manobo\", \"nahuatl\", \"swahili\", \"tagalog\"]\n",
        "    out = []\n",
        "    for doc in documents:\n",
        "      featureVector = vectorize_document(doc, feature_map, ngram_length)\n",
        "      idx = classify(W, featureVector)\n",
        "      if return_class_names == True:\n",
        "        out = np.append(out, idxToName[idx])\n",
        "      else:\n",
        "        out = np.append(out, idx)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "924RYujfyE1Q"
      },
      "source": [
        "Now, for each instance in the test set, print a tuple of the actual label, then the predicted label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPbGtDUayE1Q",
        "outputId": "71f7a147-3801-4f23-fc44-c51fe150b18f",
        "output_for": "3.1",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('swahili', 'swahili'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('hausa', 'hausa'), ('nahuatl', 'nahuatl'), ('hausa', 'hausa'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('hausa', 'swahili'), ('indonesian', 'indonesian'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('nahuatl', 'nahuatl'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('indonesian', 'indonesian'), ('nahuatl', 'nahuatl'), ('nahuatl', 'nahuatl'), ('tagalog', 'tagalog'), ('hausa', 'hausa'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('tagalog', 'swahili'), ('manobo', 'manobo'), ('manobo', 'manobo'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('indonesian', 'indonesian'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('hausa', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('nahuatl', 'nahuatl'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('manobo', 'manobo'), ('nahuatl', 'nahuatl'), ('indonesian', 'indonesian'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('nahuatl', 'nahuatl'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('hausa', 'hausa'), ('hausa', 'swahili'), ('hausa', 'hausa'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('indonesian', 'indonesian'), ('hausa', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('nahuatl', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('nahuatl', 'nahuatl'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('nahuatl', 'nahuatl'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('hausa', 'hausa'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('hausa', 'hausa'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('hausa', 'hausa'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('nahuatl', 'swahili'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('hausa', 'hausa'), ('swahili', 'swahili'), ('hausa', 'indonesian'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('indonesian', 'indonesian'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('nahuatl', 'nahuatl'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('hausa', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('nahuatl', 'nahuatl'), ('indonesian', 'indonesian'), ('manobo', 'manobo'), ('manobo', 'swahili'), ('indonesian', 'indonesian'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('hausa', 'swahili'), ('manobo', 'manobo'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('hausa', 'hausa'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('hausa', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('indonesian', 'indonesian'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('nahuatl', 'nahuatl'), ('tagalog', 'tagalog'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('hausa', 'hausa'), ('hausa', 'hausa'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('hausa', 'swahili'), ('hausa', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('indonesian', 'indonesian'), ('manobo', 'manobo'), ('nahuatl', 'nahuatl'), ('swahili', 'swahili'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili')]\n"
          ]
        }
      ],
      "source": [
        "# the output of this cell will be used for test 3.1\n",
        "test_labels, test_documents = zip(*test_observations)\n",
        "test_predictions = predict(\n",
        "    test_documents,\n",
        "    W_inspect,\n",
        "    feature_map,\n",
        "    lang_map,\n",
        "    ngram_length=INSPECTION_NGRAMS,\n",
        "    return_class_names=True,\n",
        ")\n",
        "\n",
        "print(list(zip(test_labels, test_predictions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH5VlGV2yE1Q"
      },
      "source": [
        "### Plotting a Confusion Matrix\n",
        "\n",
        " A _confusion matrix_ is a $k \\times k$ matrix, where k is your number of classes, where the cell in position $(i,j)$ counts the number of instances that belong to class $i$ that were predicted to be in class $j$. The diagonal entries represent correct classifications; anything off of the diagonal represents an incorrect classification. The example below, from the [scikit learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html), shows a confusion matrix for a binary classification problem.\n",
        "\n",
        "![A confusion matrix for a binary classification problem](confusion_matrix.png)\n",
        "\n",
        "\n",
        "Confusion matrices can be useful to see what types of errors your classifier is making. If errors are concentrated into particular cells, it could indicate the kind of data that your classifier struggles with. Write a function to take a list of test observations, and output a numpy array that represents your classifier's confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "YAjrzsrVyE1Q"
      },
      "outputs": [],
      "source": [
        "def get_confusion_matrix(\n",
        "    labels: list[str], predictions: list[str], lang_map: dict[str, np.ndarray]\n",
        ") -> np.ndarray:\n",
        "    # your code here\n",
        "    #matrix should be a list of lists, K * K matrix, np array.\n",
        "    #non diagonal are incorrect\n",
        "    #columns are unordered, just use the order of the items in the labels\n",
        "    #can use order of items in the lang map, first one is hausa.\n",
        "    #output format may get no points\n",
        "    #hausa weight is just the one row of the weights corresponding to hausa.\n",
        "    #print(lang_map)\n",
        "    K = len(lang_map)\n",
        "    cnf = np.zeros((K,K))\n",
        "    for i in range(0,len(labels)):\n",
        "      lblIdx = np.argmax(lang_map[labels[i]])\n",
        "      predIdx = np.argmax(lang_map[predictions[i]])\n",
        "      cnf[lblIdx][predIdx] += 1\n",
        "    return cnf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBT2kw9UyE1Q"
      },
      "source": [
        "Now, print your confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWUEm2gLyE1Q",
        "outputId": "e989debd-3be4-4ca2-fa0a-479c8f30ba40",
        "output_for": "3.2",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 18.,   1.,   0.,   0.,   9.,   0.],\n",
              "       [  0.,  31.,   0.,   0.,   0.,   1.],\n",
              "       [  0.,   0.,  33.,   0.,   1.,   0.],\n",
              "       [  0.,   0.,   0.,  34.,   2.,   0.],\n",
              "       [  0.,   0.,   0.,   0., 142.,   0.],\n",
              "       [  0.,   0.,   0.,   0.,   1., 120.]])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "# the output of this cell will be used for test 3.2\n",
        "get_confusion_matrix(test_labels, test_predictions, lang_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ3tsbqqyE1Q"
      },
      "source": [
        "From your confusion matrix, how many Hausa examples are misclassified as Swahili? Print each of the misclassified documents on a new line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQT9ECjIyE1R",
        "outputId": "be8e6f27-fb75-4aef-80c0-312afbe1b204",
        "output_for": "3.3",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dalilin haka kuwa sun nuna cewa ayukan da shari'a take bukata na nan a rubuce a zuciyarsu. lamirinsu kuma na yi masu shaida, tunanainsu kuma, ko dai yana kashe su, ko kuma yana karesu.\n",
            "Sai aka kawo kansa bisa tire, aka mika wa yarinyar, ta kuwa kai wa mahaifiyarta.\n",
            "Ta wurin haka Yesu ya bada tabbatacce da kuma ingataccen alkwari.\n",
            "Baku sani ba mu za mu yiwa mala'iku shari'a? Balle shari'ar al'amuran wannan rai?\n",
            "Ya ce masu, \"A rubuce yake, cewa Almasihu za ya sha wuya, zai tashi kuma daga matattu a rana ta uku.\n",
            "Bayan shekara uku na je Urushalima don in san Kefas, na zauna da shi na kwana goma sha biyar.\n",
            "Ga wani yin ayyukan iko, ga wani kuwa annabci. Ga wani kuwa an ba shi baiwar bambance ruhohi, ga wani harsuna daban daban, kuma ga wani fassarar harsuna.\n",
            "Amma da hankalinsa ya dawo, ya ce, 'Barorin mahaifina su nawa ne da suke da abinci isasshe, amma ina nan a nan, ina mutuwa sabili da yunwa!\n",
            "Wannan dabban da na gani yana kama da damisa, kafafunsa kuma kamar na beyar, bakinsa kuma kamar na zaki. Wannan diragon ya ba shi karfinsa, da kursiyinsa da ikonsa mai girma na sarauta.\n"
          ]
        }
      ],
      "source": [
        "# the output of this cell will be used for test 3.3\n",
        "# your code here\n",
        "labels = test_labels\n",
        "predictions = test_predictions\n",
        "for i in range(0,len(labels)):\n",
        "      lblIdx = np.argmax(lang_map[labels[i]])\n",
        "      predIdx = np.argmax(lang_map[predictions[i]])\n",
        "      if lblIdx == 0 and predIdx == 4:\n",
        "        print(test_documents[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxINVZHDyE1R"
      },
      "source": [
        "Can you formulate a hypothesis for why these Hausa examples are being misclassified as Swahili? Peruse the Wikipedia pages of the two languages, and inspect the data and features of the two languages. Consider reasons based in what you know about the languages, and about machine learning. Try to come up with 2-3 experiments you might run to validate your hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LarqWkqQyE1R"
      },
      "source": [
        "### Examining Feature Weights\n",
        "\n",
        "The classifier that we've built for softmax classification behaves in many ways like using several binary softmax classifiers stacked together. The $K \\times N$ feature matrix can interpreted as a $1 \\times N$ feature vector for each class. In this section, we'll examine the weights for a few of our classified languages. To get feature names out of these vectors, we'll construct an inverted feature map, that maps indices in the feature vectors back to n-grams. Then, extract the $1 \\times N$ vector that corresponds to Hausa. Print the shape of the Hausa feature vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL8Pl5iPyE1R",
        "outputId": "e18fbbf6-f9b7-4984-a856-a885b83f7802",
        "output_for": "4.1",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7942,)\n"
          ]
        }
      ],
      "source": [
        "# the output of this cell will be used for test 4.1\n",
        "# Invert the feature map\n",
        "inverted_feature_map = {v: k for k, v in feature_map.items()}\n",
        "#print(inverted_feature_map)\n",
        "hausaV = W_inspect[0]\n",
        "print(np.shape(hausaV))\n",
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l46owXayE1R"
      },
      "source": [
        "Now, let's find the features that are most strongly predictive of an instance being Hausa. From the Hausa feature vector, find the names of the features with the top-10 positive values. Print your results with a tuple of (feature name, feature_weight) on each line:\n",
        "```\n",
        "(\"aaa\", 0.04597442104739769)\n",
        "(\"bbb\", 0.03454984682736487)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALnp3khdyE1R",
        "outputId": "03996aa3-abea-4da9-d9df-4b7af99726dd",
        "output_for": "4.2",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('da!', 0.063150564082901)\n",
            "(' de', 0.054363893172131285)\n",
            "('in!', 0.027868422071017727)\n",
            "(' ye', 0.02487968026676279)\n",
            "(\"ar'\", 0.023277611165428837)\n",
            "(' si', 0.02198093247915512)\n",
            "(' be', 0.021048163450823363)\n",
            "('ai,', 0.020038068668300366)\n",
            "('ka!', 0.019952359788949974)\n",
            "('ya!', 0.01917826911896685)\n"
          ]
        }
      ],
      "source": [
        "# the output of this cell will be used for test 4.2\n",
        "# your code here\n",
        "ind = np.argpartition(hausaV, -10)[-10:]\n",
        "mjr = []\n",
        "for idx in ind:\n",
        "  mjr.append((inverted_feature_map[idx],hausaV[idx]))\n",
        "out = sorted(mjr, key=lambda x:x[1])\n",
        "out.reverse()\n",
        "for item in out:\n",
        "  print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxuk4VZryE1R"
      },
      "source": [
        "Now, for each language, print the top-10 features. Print the name of each language, and then a list of it's top-10 features on the following line, e.g.\n",
        "\n",
        "```\n",
        "hausa\n",
        "[(\"aaa\", 0.04597442104739769)...]\n",
        "indonesian\n",
        "[(\"aaa\", 0.04597442104739769)...]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flCuKMqLyE1R",
        "outputId": "f27101af-85a4-4510-c523-281d5ca309d3",
        "output_for": "4.3",
        "tags": [
          "Answer Expected"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hausa\n",
            "[('da!', 0.063150564082901), (' de', 0.054363893172131285), ('in!', 0.027868422071017727), (' ye', 0.02487968026676279), (\"ar'\", 0.023277611165428837), (' si', 0.02198093247915512), (' be', 0.021048163450823363), ('ai,', 0.020038068668300366), ('ka!', 0.019952359788949974), ('ya!', 0.01917826911896685)]\n",
            "indonesian\n",
            "[('an!', 0.06804588397633496), (' mf', 0.06296123413453153), ('ani', 0.05154754483007454), ('bes', 0.03768695995865839), (' bi', 0.037192031587281224), (' kh', 0.03452062978681125), (' sh', 0.03324158839344721), ('meo', 0.033132192231201625), (' do', 0.03245654176502971), ('ah!', 0.030398224045560432)]\n",
            "manobo\n",
            "[(' tr', 0.107483603861862), ('to!', 0.10445031656438448), (' nu', 0.07028591690295224), ('no,', 0.05550510517116152), ('an!', 0.035445216508256695), (' oj', 0.035146648956816885), ('ow,', 0.03348412435308619), ('n u', 0.02977386180101167), ('dio', 0.028821441995230877), ('on,', 0.027656726512025165)]\n",
            "nahuatl\n",
            "[('tle', 0.0634978527218824), (' to', 0.060322440722325184), ('ive', 0.03857687930949736), ('en,', 0.03835791268396555), ('va,', 0.036453474053082455), ('leo', 0.03546806924912793), (' io', 0.035139038124397115), ('tli', 0.03451515404817115), ('kei', 0.0286981326945062), (' ol', 0.02723314830481151)]\n",
            "swahili\n",
            "[('wa!', 0.1279429223309626), (' we', 0.11512051674271453), ('na!', 0.09338824051503675), ('a l', 0.09074696981732663), ('a n', 0.07502591759995446), (' nc', 0.06697104930386928), (' kw', 0.06536563489475734), (' ky', 0.05886239839638622), (' ye', 0.054507018539576305), ('ni!', 0.05409784337279098)]\n",
            "tagalog\n",
            "[('ng!', 0.20283193088819967), ('sa!', 0.08168686073388248), ('ani', 0.08045671180248927), (' se', 0.07195192125265296), (' ni', 0.06782998219125647), (' ao', 0.05930270090602767), ('ay\"', 0.05460276407961421), ('g n', 0.049187747998878435), ('at!', 0.047522415204342035), (' nc', 0.042557480893506354)]\n"
          ]
        }
      ],
      "source": [
        "# the output of this cell will be used for test 4.3\n",
        "# your code here\n",
        "idxToName = [\"hausa\", \"indonesian\", \"manobo\", \"nahuatl\", \"swahili\", \"tagalog\"]\n",
        "for i in range(0,len(idxToName)):\n",
        "  v = W_inspect[i]\n",
        "  ind = np.argpartition(v, -10)[-10:]\n",
        "  print(idxToName[i])\n",
        "  cmnftrs = []\n",
        "  for idx in ind:\n",
        "    cmnftrs.append((inverted_feature_map[idx],v[idx]))\n",
        "  out = sorted(cmnftrs, key=lambda x:x[1])\n",
        "  out.reverse()\n",
        "  print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJQuqbexyE1R"
      },
      "source": [
        "Choose one of the languages, and peruse its Wikipedia page or other reliable resources to learn a bit more about the language. Do the top 10 features in that language make sense given what you've learned about the structure of the language? Why or why not? Again, consider reasons stemming both from what you've learned about the language, as well as machine learning and multinomial logistic regression specifically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN8KW-cJyE1R"
      },
      "source": [
        "### Extra Credit\n",
        "\n",
        "Implement at least one of the experiments you devised to test your hypothesis regarding misclassification of Hausa as Swahili. In order to get credit you must submit not just the code and results, but also clearly describe your hypothesis, the experiment, and why the experiment is suitable to test your hypothesis. Full credit will be given to hypotheses and experiments that are well thought out, explained clearly and convincingly, and backed up with suitable evidence (computed or cited)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "8CjOAt7tyE1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0f58364-b794-43f0-bb08-e557bed5435f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(' ky', 0.0687249563112323, 426), ('an!', 0.06888746764311318, 2595), ('da!', 0.07662190642693825, 3072), ('a n', 0.08183085157583467, 2264), ('na!', 0.09365807851858221, 5463), ('a l', 0.08281409330134763, 2262), ('wa!', 0.13904290837497557, 7333), (' we', 0.11906075658844521, 541), (' nc', 0.08618779662358586, 455), (' de', 0.08409746274560283, 356)]\n",
            "7942 7333\n"
          ]
        }
      ],
      "source": [
        "# Implement extra credit here.\n",
        "# To try to decrease the probability of misclassification between hausa and indonesian, I attempted to find probable features in hausa that were improbable in swahili, and vice versa. This was done using the following code:\n",
        "idxToName = [\"hausa\", \"indonesian\", \"manobo\", \"nahuatl\", \"swahili\", \"tagalog\"]\n",
        "ngram_length = 3\n",
        "diffArr = np.array([])\n",
        "for i in range(0,len(W_inspect[0])):\n",
        "  diffArr = np.append(diffArr, abs(W_inspect[0][i]-W_inspect[4][i]))\n",
        "amax = np.argmax(diffArr)\n",
        "ind = np.argpartition(diffArr, -10)[-10:]\n",
        "#print(idxToName[i])\n",
        "cmnftrs = []\n",
        "for idx in ind:\n",
        "  cmnftrs.append((inverted_feature_map[idx],diffArr[idx], idx))\n",
        "print(cmnftrs)\n",
        "print(len(diffArr), amax)\n",
        "#print(W_inspect[0][7333], W_inspect[4][7333])\n",
        "\n",
        "#After these are computed, we can change our classify function to look for these specific features when\n",
        "#the probability of swahili and probability of hausa are similar\n",
        "#this method may help in resolving knifes edge issues where the model comes \"close\" to classifying correctly,\n",
        "#say use this method when the probability of hausa and swahili are within 5% of each other."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('nlphw03')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9b5c567e963c4c8a4f65edac5dbbc9d8f011f564e4ce8a416ba289a7b299ae77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}